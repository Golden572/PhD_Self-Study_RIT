{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46d284c0",
   "metadata": {},
   "source": [
    "### MNIST Digits Classification with NN - Siddhardhan "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f79c45",
   "metadata": {},
   "source": [
    "#### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9ebf8b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Importing Libraries\n",
    "from PIL import Image\n",
    "import cv2\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Random Seed\n",
    "my_seed = 2\n",
    "torch.manual_seed(my_seed)\n",
    "np.random.seed(my_seed)\n",
    "\n",
    "# Cuda Device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c995a0",
   "metadata": {},
   "source": [
    "#### Data Collection & Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "84a587c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Collection\n",
    "train_set = datasets.MNIST(root='./data', train=True, download=True)\n",
    "test_set = datasets.MNIST(root='./data', train=False, download=True)\n",
    "\n",
    "# As Arrays\n",
    "X_train, y_train = np.asarray(train_set.data), np.asarray(train_set.targets)\n",
    "X_test, y_test = np.asarray(test_set.data), np.asarray(test_set.targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc1fe72b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datsets Shape:\n",
      "X_train.shape=(60000, 28, 28)\n",
      "y_train.shape=(60000,)\n",
      "X_test.shape=(10000, 28, 28)\n",
      "y_test.shape=(10000,)\n"
     ]
    }
   ],
   "source": [
    "# Datasets Shapes\n",
    "print('Datsets Shape:')\n",
    "print(f'{X_train.shape=}'); print(f'{y_train.shape=}'); print(f'{X_test.shape=}'); print(f'{y_test.shape=}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "77c2c7b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image #25 --> Class: 2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(np.float64(-0.5), np.float64(27.5), np.float64(27.5), np.float64(-0.5))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 200x200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n_img = 25\n",
    "print(f'Image #{n_img} --> Class: {y_train[n_img]}')\n",
    "plt.figure(figsize=(2,2)); plt.imshow(X_train[n_img]); plt.axis('off')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01fa28ff",
   "metadata": {},
   "source": [
    "##### EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "215837dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "targets\n",
       "1          6742\n",
       "7          6265\n",
       "3          6131\n",
       "2          5958\n",
       "9          5949\n",
       "0          5923\n",
       "6          5918\n",
       "8          5851\n",
       "4          5842\n",
       "5          5421\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Target Count\n",
    "y_train_df = pd.DataFrame(y_train, columns=['targets'])\n",
    "y_train_df.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bb7014e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resize and Scale Images\n",
    "\n",
    "# In this case, all images alraady are the same size.\n",
    "\n",
    "# Scaling\n",
    "X_train_scaled = X_train/255\n",
    "X_test_scaled = X_test/255\n",
    "\n",
    "# Transform to Tensor\n",
    "X_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float).to(device)\n",
    "y_train_tensor = torch.tensor(y_train, dtype= torch.long).to(device)\n",
    "X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float).to(device)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9db12c73",
   "metadata": {},
   "source": [
    "#### Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cab83596",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Pytorch\n",
    "\n",
    "# Torch Seed & Device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "399871bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Structure, Training Loop Function & Testing Function\n",
    "\n",
    "# Model Structure\n",
    "\n",
    "# Training Loop Function\n",
    "\n",
    "# Testing Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "228f5751",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' [GU√çA - NO MIRAR] Model Structure, Training Loop Function & Testing Function\\nclass Simple_Neural_Network(torch.nn.Module):\\n    def __init__(self, input_shape, n_neurons, output_shape):\\n        super().__init__()\\n        self.flatten_1 = torch.nn.Flatten()\\n        self.layer_1 = torch.nn.Linear(input_shape, n_neurons)\\n        self.relu_1 = torch.nn.ReLU()\\n        self.layer_2 = torch.nn.Linear(n_neurons, output_shape)\\n        self.sigmoid = torch.nn.Sigmoid()\\n\\n    def forward(self, x):\\n        x = self.flatten_1(x)\\n        x  = self.relu_1(self.layer_1(x))\\n        x  = self.sigmoid(self.layer_2(x))\\n        return x\\n\\n# Training Loop\\ndef train_model(my_model, train_loader, val_loader, criterion, optimizer, epochs=100, patience=10):\\n    \\n    # Early Stopping Parameters - Initialization\\n    best_loss = float(\\'inf\\')\\n    trigger_times = 0\\n    \\n    # List for Plotting Losses per Epoch\\n    train_epoch_losses = []\\n    val_epoch_losses = []\\n\\n    for epoch in range (epochs):\\n        \\n        my_model.train() # Put model in training mode\\n        epoch_loss = 0      # Total Epoch Loss\\n        running_loss = 0.0  # Total Running Loss until a given Epoch\\n        correct = 0\\n        \\n        for x_batch, y_batch in train_loader:\\n            x_batch, y_batch = x_batch.to(device), y_batch.to(device)\\n            \\n            optimizer.zero_grad()               # Insert Explanation\\n            outputs = my_model(x_batch)         # Feed Forward\\n            loss = criterion(outputs, y_batch)  # Error Calculation\\n            loss.backward()                     # Backpropagation\\n            optimizer.step()                    # Update weights\\n            \\n            running_loss = running_loss + loss.item()           # Calculate Running Loss until this Epoch\\n            preds = outputs.argmax(dim=1)                       # Make the predictions for this Batch\\n            correct = correct + (preds == y_batch).sum().item() # Caclulate Total Correct Predictions in this Epoch\\n        \\n        epoch_loss = running_loss / len(train_loader.dataset); train_epoch_losses.append(epoch_loss)\\n        accuracy = correct / len(train_loader.dataset)\\n        \\n        # Validation\\n        my_model.eval()\\n        val_epoch_loss = 0 \\n        val_running_loss = 0\\n        val_correct = 0\\n        \\n        with torch.no_grad():\\n            for x_val, y_val in val_loader:\\n                x_val, y_val = x_val.to(device), y_val.to(device)\\n                val_outputs = my_model(x_val)               # Validation Feed Forward\\n                val_loss = criterion(val_outputs, y_val)    # Validation Error Calculation\\n\\n                val_running_loss = val_running_loss + val_loss.item()   # Validation Running Loss until this Epoch\\n                val_preds = val_outputs.argmax(dim=1)                  # Make Validaiton Predictions for this Batch\\n                val_correct = val_correct + (val_preds == y_val).sum().item()\\n\\n        val_epoch_loss = val_running_loss / len(val_loader.dataset); val_epoch_losses.append(val_epoch_loss)\\n        val_accuracy = val_correct / len(val_loader.dataset)\\n        \\n        # User Info\\t\\t\\t\\t\\n        if (epoch + 1) % max(1, epochs // 10) == 0:\\n                print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {epoch_loss:.4f}, Acc: {accuracy:.4f}, Val Loss: {val_epoch_loss:.4f}, Val Acc: {val_accuracy:.4f}\")\\n        \\n        # Early Stopping & Saving Best Model\\n        if (epoch_loss <= best_loss):\\n                best_loss = epoch_loss\\n                trigger_times = 0\\n                torch.save(my_model.state_dict(), f\\'best_model_breast_cancer.pth\\')\\n        else:\\n            trigger_times += 1\\n            if (trigger_times >= patience):\\n                print(f\\'Early stopping at epoch {epoch+1}\\')\\n                break\\n    my_model.load_state_dict(torch.load(f\\'best_model_breast_cancer.pth\\'))\\n\\n    # Plotting\\n    plt.figure()\\n    plt.plot(train_epoch_losses, label=\\'Train Loss\\')\\n    plt.plot(val_epoch_losses, label=\\'Val Loss\\')\\n    plt.xlabel(\"Epoch\")\\n    plt.ylabel(\"Loss\")\\n    plt.title(\"Training & Validation Loss\")\\n    plt.legend()\\n    plt.grid(True)\\n    plt.show()\\n\\ndef test_model(my_model, test_loader, criterion):\\n    my_model.eval()\\n      \\n    test_running_loss = 0\\n    test_correct = 0\\n    all_preds = []\\n    all_labels = []\\n\\n    with torch.no_grad():\\n        for x_batch, y_batch in test_loader:           \\n            x_batch, y_batch = x_batch.to(device), y_batch.to(device)\\n            outputs = my_model(x_batch)\\n            loss = criterion(outputs, y_batch)\\n            test_running_loss = test_running_loss + loss.item()\\n            test_preds = outputs.argmax(dim=1)\\n            test_correct = test_correct + (test_preds == y_batch).sum().item()\\n    \\n        avg_test_loss = test_running_loss / len(test_loader.dataset)\\n        test_accuracy = test_correct / len(test_loader.dataset)\\n\\n        all_preds.extend(test_preds.cpu().numpy())\\n        all_labels.extend(y_batch.cpu().numpy())\\n\\n        cm = confusion_matrix(all_labels, all_preds)\\n        ConfusionMatrixDisplay(cm).plot()\\n\\n        print(f\"\\nüß™ Test Loss: {avg_test_loss:.4f}\")\\n        print(f\"‚úÖ Test Accuracy: {test_accuracy * 100:.2f}%\")'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# [GU√çA - NO MIRAR] Model Structure, Training Loop Function & Testing Function\n",
    "''' [GU√çA - NO MIRAR] Model Structure, Training Loop Function & Testing Function\n",
    "class Simple_Neural_Network(torch.nn.Module):\n",
    "    def __init__(self, input_shape, n_neurons, output_shape):\n",
    "        super().__init__()\n",
    "        self.flatten_1 = torch.nn.Flatten()\n",
    "        self.layer_1 = torch.nn.Linear(input_shape, n_neurons)\n",
    "        self.relu_1 = torch.nn.ReLU()\n",
    "        self.layer_2 = torch.nn.Linear(n_neurons, output_shape)\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten_1(x)\n",
    "        x  = self.relu_1(self.layer_1(x))\n",
    "        x  = self.sigmoid(self.layer_2(x))\n",
    "        return x\n",
    "\n",
    "# Training Loop\n",
    "def train_model(my_model, train_loader, val_loader, criterion, optimizer, epochs=100, patience=10):\n",
    "    \n",
    "    # Early Stopping Parameters - Initialization\n",
    "    best_loss = float('inf')\n",
    "    trigger_times = 0\n",
    "    \n",
    "    # List for Plotting Losses per Epoch\n",
    "    train_epoch_losses = []\n",
    "    val_epoch_losses = []\n",
    "\n",
    "    for epoch in range (epochs):\n",
    "        \n",
    "        my_model.train() # Put model in training mode\n",
    "        epoch_loss = 0      # Total Epoch Loss\n",
    "        running_loss = 0.0  # Total Running Loss until a given Epoch\n",
    "        correct = 0\n",
    "        \n",
    "        for x_batch, y_batch in train_loader:\n",
    "            x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()               # Insert Explanation\n",
    "            outputs = my_model(x_batch)         # Feed Forward\n",
    "            loss = criterion(outputs, y_batch)  # Error Calculation\n",
    "            loss.backward()                     # Backpropagation\n",
    "            optimizer.step()                    # Update weights\n",
    "            \n",
    "            running_loss = running_loss + loss.item()           # Calculate Running Loss until this Epoch\n",
    "            preds = outputs.argmax(dim=1)                       # Make the predictions for this Batch\n",
    "            correct = correct + (preds == y_batch).sum().item() # Caclulate Total Correct Predictions in this Epoch\n",
    "        \n",
    "        epoch_loss = running_loss / len(train_loader.dataset); train_epoch_losses.append(epoch_loss)\n",
    "        accuracy = correct / len(train_loader.dataset)\n",
    "        \n",
    "        # Validation\n",
    "        my_model.eval()\n",
    "        val_epoch_loss = 0 \n",
    "        val_running_loss = 0\n",
    "        val_correct = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for x_val, y_val in val_loader:\n",
    "                x_val, y_val = x_val.to(device), y_val.to(device)\n",
    "                val_outputs = my_model(x_val)               # Validation Feed Forward\n",
    "                val_loss = criterion(val_outputs, y_val)    # Validation Error Calculation\n",
    "\n",
    "                val_running_loss = val_running_loss + val_loss.item()   # Validation Running Loss until this Epoch\n",
    "                val_preds = val_outputs.argmax(dim=1)                  # Make Validaiton Predictions for this Batch\n",
    "                val_correct = val_correct + (val_preds == y_val).sum().item()\n",
    "\n",
    "        val_epoch_loss = val_running_loss / len(val_loader.dataset); val_epoch_losses.append(val_epoch_loss)\n",
    "        val_accuracy = val_correct / len(val_loader.dataset)\n",
    "        \n",
    "        # User Info\t\t\t\t\n",
    "        if (epoch + 1) % max(1, epochs // 10) == 0:\n",
    "                print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {epoch_loss:.4f}, Acc: {accuracy:.4f}, Val Loss: {val_epoch_loss:.4f}, Val Acc: {val_accuracy:.4f}\")\n",
    "        \n",
    "        # Early Stopping & Saving Best Model\n",
    "        if (epoch_loss <= best_loss):\n",
    "                best_loss = epoch_loss\n",
    "                trigger_times = 0\n",
    "                torch.save(my_model.state_dict(), f'best_model_breast_cancer.pth')\n",
    "        else:\n",
    "            trigger_times += 1\n",
    "            if (trigger_times >= patience):\n",
    "                print(f'Early stopping at epoch {epoch+1}')\n",
    "                break\n",
    "    my_model.load_state_dict(torch.load(f'best_model_breast_cancer.pth'))\n",
    "\n",
    "    # Plotting\n",
    "    plt.figure()\n",
    "    plt.plot(train_epoch_losses, label='Train Loss')\n",
    "    plt.plot(val_epoch_losses, label='Val Loss')\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Training & Validation Loss\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "def test_model(my_model, test_loader, criterion):\n",
    "    my_model.eval()\n",
    "      \n",
    "    test_running_loss = 0\n",
    "    test_correct = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x_batch, y_batch in test_loader:           \n",
    "            x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "            outputs = my_model(x_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            test_running_loss = test_running_loss + loss.item()\n",
    "            test_preds = outputs.argmax(dim=1)\n",
    "            test_correct = test_correct + (test_preds == y_batch).sum().item()\n",
    "    \n",
    "        avg_test_loss = test_running_loss / len(test_loader.dataset)\n",
    "        test_accuracy = test_correct / len(test_loader.dataset)\n",
    "\n",
    "        all_preds.extend(test_preds.cpu().numpy())\n",
    "        all_labels.extend(y_batch.cpu().numpy())\n",
    "\n",
    "        cm = confusion_matrix(all_labels, all_preds)\n",
    "        ConfusionMatrixDisplay(cm).plot()\n",
    "\n",
    "        print(f\"\\nüß™ Test Loss: {avg_test_loss:.4f}\")\n",
    "        print(f\"‚úÖ Test Accuracy: {test_accuracy * 100:.2f}%\")'''          \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "04e6a735",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters & Training Inputs\n",
    "\n",
    "# Hyperparameters\n",
    "\n",
    "# Create X and y as Tensors\n",
    "\n",
    "# Create Tensor Datasets (train, validation, test)\n",
    "\n",
    "# Dataloaders for Batches (train, validation, test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "684fdd12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the Neural Network (Model, Criterion and Optimizer)\n",
    "\n",
    "# Train Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7e5c2681",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
